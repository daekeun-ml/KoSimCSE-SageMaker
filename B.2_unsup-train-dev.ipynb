{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e58277-032e-43fe-a551-b7ffc60ed2e0",
   "metadata": {},
   "source": [
    "# KoSimCSE training (Unsupervised) \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d8b63-dd15-46d3-9054-e792e77ee6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "# from dataclasses import dataclass\n",
    "# from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from more_itertools import chunked\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, logging\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_utils import BatchEncoding, PreTrainedTokenizer\n",
    "\n",
    "from src.sts import STSEvaluation\n",
    "from src.simcse import SimCSEDatasetFromHF, SimCSEModel, set_seed\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse the arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # path\n",
    "    parser.add_argument(\"--base_model\", type=str, default=\"klue/roberta-base\", help=\"Model id to use for training.\")\n",
    "    parser.add_argument(\"--dataset_dir\", type=str, default=\"./datasets/unsup-simcse\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./unsup-model\")\n",
    "\n",
    "    # add training hyperparameters\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size to use for training.\")\n",
    "    # the number of epochs is 1 for Unsup-SimCSE, and 3 for Sup-SimCSE in the paper\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-5, help=\"Learning rate to use for training.\")\n",
    "    parser.add_argument(\"--num_warmup_steps\", type=int, default=0)\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.05) # see Table D.1 of the paper\n",
    "    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"linear\")\n",
    "\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval_steps\", type=int, default=50)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\")\n",
    "    parser.add_argument(\"--debug\", default=True, action=\"store_true\")    \n",
    "    \n",
    "    args = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd49be5-7c8a-4cd5-b892-403c17b7b02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args, _ = parse_args()\n",
    "args.base_model = \"klue/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669c874-0a82-4a36-b789-7b78265fa6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "set_seed(args.seed)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"heegyu/kowiki-sentences\", split=\"train\")\n",
    "\n",
    "if args.debug:\n",
    "    train_num_samples = 2000\n",
    "    dataset = dataset.shuffle(seed=42).select(range(train_num_samples))\n",
    "\n",
    "train_dataset = SimCSEDatasetFromHF(dataset)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
    "model = SimCSEModel(args.base_model).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea921b-8998-428a-bf3c-5191a718a1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "def collate_fn(batch: List[str]) -> BatchEncoding:\n",
    "    return tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=args.max_seq_len,\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers and pin_memory are for speeding up training\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    # batch_size varies in the last batch because\n",
    "    # the last batch size will be the number of remaining samples (i.e. len(train_dataloader) % batch_size)\n",
    "    # to avoid unstablity of contrastive learning, we drop the last batch\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ea1f5-9ae6-4a8a-9e9a-cf91224a7e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "# reference implementation uses a linear scheduler with warmup, which is a default scheduler of transformers' Trainer\n",
    "# with num_training_steps = 0 (i.e. no warmup)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    # len(train_dataloader) is the number of steps in one epoch\n",
    "    num_training_steps=len(train_dataloader) * args.num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89e851-1506-459c-9725-cd7d4c7d9ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sts = STSEvaluation()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode(texts: List[str]) -> torch.Tensor:\n",
    "    embs = []\n",
    "    model.eval()\n",
    "    for text in chunked(texts, args.batch_size * 8):\n",
    "        batch: BatchEncoding = tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # SimCSE uses MLP layer only during training\n",
    "        # in this implementation, we use `model.training` to switch between training and evaluation\n",
    "        emb = model(**batch.to(args.device))\n",
    "        embs.append(emb.cpu())\n",
    "    # shape of output: (len(texts), hidden_size)\n",
    "    return torch.cat(embs, dim=0)\n",
    "\n",
    "# evaluation before training\n",
    "model.eval()\n",
    "best_sts = sts.dev(encode=encode)[\"avg\"]\n",
    "best_step = 0\n",
    "\n",
    "# evaluate the model and store metrics before training\n",
    "# this is important to check the appropriateness of training procedure\n",
    "print(f\"epoch: {0:>3};\\tstep: {0:>6};\\tloss: {' '*9}nan;\\tAvg. STS: {best_sts:.5f};\")\n",
    "logs: List[Dict[str, Union[int, float]]] = [\n",
    "    {\n",
    "        \"epoch\": 0,\n",
    "        \"step\": best_step,\n",
    "        \"loss\": None,\n",
    "        \"sts\": best_sts,\n",
    "    }\n",
    "]\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # tqdm makes it easy to visualize how well the training is progressing\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader),\n",
    "        total=len(train_dataloader),\n",
    "        dynamic_ncols=True,\n",
    "    ):\n",
    "        # transfer batch to the device\n",
    "        batch: BatchEncoding = batch.to(args.device)\n",
    "        # if you want to see the actual data, please uncomment the following line.\n",
    "        # print(batch)\n",
    "        # and also, if you want to see the actual input strings, please uncomment the following line.\n",
    "        # print(tokenizer.batch_decode(batch.input_ids, skip_special_tokens=True))\n",
    "\n",
    "        # simply forward inputs twice!\n",
    "        # different dropout masks are adapt automatically\n",
    "        emb1 = model.forward(**batch)\n",
    "        emb2 = model.forward(**batch)\n",
    "\n",
    "        # SimCSE training objective:\n",
    "        #    maximize the similarity between the same sentence\n",
    "        # => make diagonal elements most similar\n",
    "\n",
    "        # shape of sim_matrix: (batch_size, batch_size)\n",
    "        # calculate cosine similarity between all pair of embeddings (n x n)\n",
    "        sim_matrix = F.cosine_similarity(emb1.unsqueeze(1), emb2.unsqueeze(0), dim=-1)\n",
    "        # FYI: SimCSE is sensitive for the temperature parameter.\n",
    "        # see Table D.1 of the paper\n",
    "        sim_matrix = sim_matrix / args.temperature\n",
    "\n",
    "        # labels := [0, 1, 2, ..., batch_size - 1]\n",
    "        # labels indicate the index of the diagonal element (i.e. positive examples)\n",
    "        labels = torch.arange(args.batch_size).long().to(args.device)\n",
    "        # it may seem strange to use Cross-Entropy Loss here.\n",
    "        # this is a shorthund of doing SoftMax and maximizing the similarity of diagonal elements\n",
    "        loss = F.cross_entropy(sim_matrix, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for every `args.eval_steps` steps, perform evaluation on STS task and print logs\n",
    "        if (step + 1) % args.eval_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "            model.eval()\n",
    "            # evaluate on the STS-B development set\n",
    "            sts_score = sts.dev(encode=encode)[\"avg\"]\n",
    "\n",
    "            # you should use the best model for the evaluation to avoid using overfitted model\n",
    "            # FYI: https://github.com/princeton-nlp/SimCSE/issues/62\n",
    "            if best_sts < sts_score:\n",
    "                best_sts = sts_score\n",
    "                best_step = step + 1\n",
    "                # only save the best performing model\n",
    "                torch.save(model.state_dict(), f\"{args.output_dir}/model.pt\")\n",
    "\n",
    "            # use `tqdm.write` instead of `print` to prevent terminal display corruption\n",
    "            tqdm.write(\n",
    "                f\"epoch: {epoch:>3};\\tstep: {step+1:>6};\\tloss: {loss.item():.10f};\\tAvg. STS: {sts_score:.5f};\"\n",
    "            )\n",
    "            logs.append(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": step + 1,\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"sts\": sts_score,\n",
    "                }\n",
    "            )\n",
    "            pd.DataFrame(logs).to_csv(f\"{args.output_dir}/logs.csv\", index=False)\n",
    "\n",
    "            # if you want to see the changes of similarity matrix, uncomment the following line\n",
    "            # tqdm.write(str(sim_matrix))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cecdaa-6248-4d97-8914-565cf2248c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save num_epochs, steps, losses, and STS dev scores\n",
    "with open(f\"{args.output_dir}/dev-metrics.json\", \"w\") as f:\n",
    "    data = {\n",
    "        \"best-step\": best_step,\n",
    "        \"best-sts\": best_sts,\n",
    "    }\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(f\"{args.output_dir}/config.json\", \"w\") as f:\n",
    "    data = {k: v if type(v) in [int, float] else str(v) for k, v in vars(args).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ef112-9885-4417-9840-1c79f045815a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = pd.read_csv(f\"{args.output_dir}/logs.csv\")\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15, 5)})\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "sns.lineplot(data=log, x=\"step\", y=\"loss\", ax=axes[0]).set(title=\"Loss\")\n",
    "sns.lineplot(data=log, x=\"step\", y=\"sts\", ax=axes[1]).set(title=\"Avg. STS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521705b-071a-4092-9d19-74b07303c8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.infer import show_embedding_score\n",
    "\n",
    "model = model.eval().cpu()\n",
    "sentences = ['이번 주 일요일에 분당 이마트 점은 문을 여나요?',\n",
    "             '일요일에 분당 이마트는 문 열어요?',\n",
    "             '분당 이마트 점은 토요일에 몇 시까지 하나요']\n",
    "show_embedding_score(tokenizer, model, sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
